{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1023
    },
    "colab_type": "code",
    "id": "v7grHkJrFoTz",
    "outputId": "cf8df5e2-0979-48c9-dc86-07bcccc56b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 2.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
      "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "Successfully installed numpy-1.15.4\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.20.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.15.4)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.7.post4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.15.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.1.0)\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download('punkt')\n",
    "!python -m spacy download en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "kQlW8062FrCW",
    "outputId": "80157bfe-9f08-45cf-f01d-9cc598b0df8e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ac75c3c389d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/Deep_Learning/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Deep_Learning/'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/Deep_Learning/\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import gc\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "import datetime as dt\n",
    "from nltk.util import ngrams\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.stem import PorterStemmer\n",
    "spacy_nlp=spacy.load('en')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##A function to preprocess the data\n",
    "def pre_process(x,remove_stopwords=False):\n",
    "    words = nltk.word_tokenize(x)\n",
    "    words = [word.lower() for word in words if ((word.isalpha())|word.isnumeric())]\n",
    "    words=list(map(lemmatizer.lemmatize, words))\n",
    "    if remove_stopwords: \n",
    "        words=  [w for w in words if w not in stop_words]\n",
    "        return words\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fZr4Sq0FFvTQ"
   },
   "outputs": [],
   "source": [
    "#######Creating some functions for feature engineering###########\n",
    "\n",
    "##Question mark feature\n",
    "def question_mark(x):\n",
    "    if \"?\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "   \n",
    "    \n",
    "##Wh-words Feature\n",
    "def wh_words(x):\n",
    "    wh=['what','when','why','which','who','how','whose','whom','where']\n",
    "    if len(list(set(x).intersection(wh)))!=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    \n",
    "##Extracting root word of the sentence   \n",
    "def root_word(x):\n",
    "    lemma=[]\n",
    "    temp=[sent.root for sent in en_nlp(x).sents]\n",
    "    for i in temp:\n",
    "        lemma.append(lemmatizer.lemmatize(str(i).lower()))\n",
    "    return list(set(lemma))\n",
    "   \n",
    "\n",
    "\n",
    "##Creating N Grams\n",
    "def create_ngrams(x,n):\n",
    "    n_grams=[]\n",
    "    for i in ngrams(x,n):\n",
    "        n_grams.append(i)\n",
    "    return list(set(n_grams))\n",
    "    \n",
    "\n",
    "    \n",
    "##Creating pos tags\n",
    "def pos_tags(x):\n",
    "    pos=nltk.pos_tag(x)\n",
    "    pos=list(set(pos))\n",
    "    return pos\n",
    "\n",
    "\n",
    "##POS tag of wh-word\n",
    "def wh_pos(x):\n",
    "    pos=nltk.pos_tag(x)\n",
    "    pos=list(set(pos))\n",
    "    for i in pos:\n",
    "        if i[1] in['WDT','WP','WP$','WRB']:\n",
    "            return str(i[1])\n",
    "        return \"None\"           \n",
    "    \n",
    "    \n",
    "    \n",
    "##Has count words or not\n",
    "def count_words(x):\n",
    "    counting_words=['number','average','many','often','much','percent','percentage','ratio','distance',\n",
    "                       'far','close','long']\n",
    "    x=list(map(str,(x)))\n",
    "    return len(set(x).intersection(counting_words))\n",
    "    \n",
    "\n",
    "##Get named entities\n",
    "def ner(self,x):\n",
    "    tagged=[]\n",
    "    document=spacy_nlp(x)\n",
    "    for element in document.ents:\n",
    "        tagged.append((element,element.label_))\n",
    "    return list(set(tagged))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GJ1luPhmFrzt"
   },
   "outputs": [],
   "source": [
    "#Initialize Global variables \n",
    "docIDFDict = {}\n",
    "avgDocLength = 0\n",
    "\n",
    "##Function to get passage corpus\n",
    "def Get_Corpus(inputfile,corpusfile):\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\")\n",
    "    fw = open(corpusfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        passage = line.strip().lower().split(\"\\t\")[2]\n",
    "        fw.write(passage+\"\\n\")\n",
    "    f.close()\n",
    "    fw.close()\n",
    "\n",
    "    \n",
    "    \n",
    "##Function to generate IDF scores using the created corpus file    \n",
    "def IDF_Generator(corpusfile, delimiter=' ', base=math.e) :\n",
    "\n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    docFrequencyDict = {}       \n",
    "    numOfDocuments = 0   \n",
    "    totalDocLength = 0\n",
    "\n",
    "    for line in open(corpusfile,\"r\",encoding=\"utf-8\") :\n",
    "        doc = line.strip().split(delimiter)\n",
    "        totalDocLength += len(doc)\n",
    "\n",
    "        doc = list(set(doc)) # Take all unique words\n",
    "\n",
    "        for word in doc : #Updates n(q_i) values for all the words(q_i)\n",
    "            if word not in docFrequencyDict :\n",
    "                docFrequencyDict[word] = 0\n",
    "            docFrequencyDict[word] += 1\n",
    "\n",
    "        numOfDocuments = numOfDocuments + 1\n",
    "        if (numOfDocuments%5000==0):\n",
    "            print(numOfDocuments)                \n",
    "\n",
    "    for word in docFrequencyDict:  #Calculate IDF scores for each word(q_i)\n",
    "        docIDFDict[word] = math.log((numOfDocuments - docFrequencyDict[word] +0.5) / (docFrequencyDict[word] + 0.5), base) #Why are you considering \"numOfDocuments - docFrequencyDict[word]\" instead of just \"numOfDocuments\"\n",
    "\n",
    "    avgDocLength = totalDocLength / numOfDocuments\n",
    "\n",
    "    print(\"NumOfDocuments : \", numOfDocuments)\n",
    "    print(\"AvgDocLength : \", avgDocLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0lYKttRPFvOq"
   },
   "outputs": [],
   "source": [
    "\n",
    "##Creating a similarity score based on BM25 formula\n",
    "def GetBM25Score(x ,k1=1.5, b=0.75, delimiter=' ') :\n",
    "    \n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    query_words= x['clean_query'].strip().lower().split(delimiter)\n",
    "    passage_words = x['clean_passage'].strip().lower().split(delimiter)\n",
    "    passageLen = len(passage_words)\n",
    "    docTF = {}\n",
    "    for word in set(query_words):   #Find Term Frequency of all query unique words\n",
    "        docTF[word] = passage_words.count(word)\n",
    "    commonWords = set(query_words) & set(passage_words)\n",
    "    tmp_score = []\n",
    "    for word in commonWords :   \n",
    "        numer = (docTF[word] * (k1+1))   #Numerator part of BM25 Formula\n",
    "        denom = ((docTF[word]) + k1*(1 - b + b*passageLen/avgDocLength)) #Denominator part of BM25 Formula \n",
    "        if(word in docIDFDict) :\n",
    "            tmp_score.append(docIDFDict[word] * numer / denom)\n",
    "\n",
    "    score = sum(tmp_score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vv41nANDIy7R"
   },
   "outputs": [],
   "source": [
    "##Creating a match function \n",
    "def match(list1,list2):\n",
    "    x=list(map(str, list1))\n",
    "    y=list(map(str,list2))\n",
    "    return len(set(x).intersection(y))\n",
    "    \n",
    "\n",
    "##Creating function for jaccardian similarity     \n",
    "def get_jaccard_sim(list1, list2):\n",
    "    a = set(list1) \n",
    "    b = set(list2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nbkXzaRNJh6f"
   },
   "outputs": [],
   "source": [
    "glove_embeddings={}\n",
    "emb_dim=100\n",
    "def load_embeddings():\n",
    "    temp={}\n",
    "    global glove_embeddings,emb_dim\n",
    "    file=open(\"glove.6B.100d.txt\",\"r\", encoding='utf-8',errors='ignore')\n",
    "    for line in file:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \".join(vec)\n",
    "        temp[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros.\n",
    "        temp[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    file.close()\n",
    "    for key in temp.keys():\n",
    "        array=temp[key].split()\n",
    "        array=list(map(float, array))\n",
    "        glove_embeddings[key]=array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "m-0pzzThJiCD"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(x):\n",
    "    embeddings=[]\n",
    "    for i in x:\n",
    "        if i in glove_embeddings:\n",
    "            embeddings.append(glove_embeddings[i])\n",
    "        else:\n",
    "            embeddings.append(glove_embeddings['zerovec'])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iffRGi2SFvYE",
    "outputId": "0d97337a-5814-425e-d7c3-338554ed5ce5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data=pd.read_csv('data.tsv',delimiter='\\t', header=None)\n",
    "data.columns=['query_id','query','passage','label','passage_id']\n",
    "subset=data.iloc[0:500000,:]\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kSLwcDrUGQBQ",
    "outputId": "d1d08137-2b45-4922-f38a-9691f2ae068e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test=pd.read_csv('eval1_unlabelled.tsv',delimiter='\\t', header=None)\n",
    "test.columns=['query_id','query','passage','passage_id']\n",
    "subset=test\n",
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Sxo9dgJJFvmz"
   },
   "outputs": [],
   "source": [
    "##Question features\n",
    "col1=features_1()\n",
    "col2=features_2()\n",
    "print(\"step 0\")\n",
    "GetCorpus('data.tsv','corpus.tsv')\n",
    "IDF_Generator('corpus.tsv', delimiter=' ', base=math.e)\n",
    "print(\"step1 \")\n",
    "subset['ques_new']=subset['query'].apply(lambda x : col1.pre_process(x,remove_stopwords=True))\n",
    "subset['ques_new_ws']=subset['query'].apply(lambda x : col1.pre_process(x,remove_stopwords=False))\n",
    "print(\"step2 \")\n",
    "subset['sent_new']=subset['passage'].apply(lambda x : col1.pre_process(x,remove_stopwords=True))\n",
    "subset['sent_new_ws']=subset['passage'].apply(lambda x : col1.pre_process(x,remove_stopwords=False))\n",
    "subset['ques_mark']=subset['query'].apply(lambda x: col1.question_mark(x))\n",
    "print(\"step3 \")\n",
    "subset['ques_length']=subset['ques_new'].apply(lambda x: len(x))\n",
    "print(\"step3 \")\n",
    "subset['wh_flag']=subset['ques_new_ws'].apply(lambda x: col1.wh_words(x))\n",
    "print(\"step4 \")\n",
    "#subset['ques_root_word']=subset['query'].apply(lambda x: col1.root_word(x))\n",
    "print(\"step5 \")\n",
    "subset['ques_unigram']=subset['ques_new'].apply(lambda x: col1.create_ngrams(x,1))\n",
    "print(\"step6 \")\n",
    "subset['ques_bigram']=subset['ques_new'].apply(lambda x: col1.create_ngrams(x,2))\n",
    "print(\"step7 \")\n",
    "subset['ques_trigram']=subset['ques_new'].apply(lambda x: col1.create_ngrams(x,3))\n",
    "#print(\"step8 \")\n",
    "#subset['ques_pos_tags']=subset['ques_new'].apply(lambda x: col1.pos_tags(x))\n",
    "#print(\"step9 \")\n",
    "#subset['ques_ner']=subset['query'].apply(lambda x : col.ner(x))\n",
    "print(\"step10 \")\n",
    "subset['ques_cnt_wrds']=subset['ques_new'].apply(lambda x: col1.count_words(x))\n",
    "#print(\"step11 \")\n",
    "#subset['wh_tag']=subset['ques_new_ws'].apply(lambda x: col1.wh_pos(x))\n",
    "print(\"step12 \")\n",
    "#subset['bm25']=subset.apply(lambda x: GetBM25Score(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kUbjfUuXFvrZ"
   },
   "outputs": [],
   "source": [
    "##Sentence features\n",
    "print(\"step1 \")\n",
    "subset['sent_length']=subset['sent_new'].apply(lambda x: len(x))\n",
    "print(\"step2 \")\n",
    "#subset['sent_root_word']=subset['passage'].apply(lambda x: col1.root_word(x))\n",
    "print(\"step3 \")\n",
    "subset['sent_unigram']=subset['sent_new'].apply(lambda x: col1.create_ngrams(x,1))\n",
    "print(\"step4 \")\n",
    "subset['sent_bigram']=subset['sent_new'].apply(lambda x: col1.create_ngrams(x,2))\n",
    "print(\"step5 \")\n",
    "subset['sent_trigram']=subset['sent_new'].apply(lambda x: col1.create_ngrams(x,3))\n",
    "#print(\"step6 \")\n",
    "#subset['sent_pos_tags']=subset['sent_new'].apply(lambda x: col1.pos_tags(x))\n",
    "#print(\"step7 \")\n",
    "#subset['sent_ner']=subset['passage'].apply(lambda x : col1.ner(x))\n",
    "print(\"step8 \")\n",
    "subset['numeric_flag']=subset.apply(lambda x: 1 if ((x['ques_cnt_wrds']>0)&(len([i for i in x['sent_new'] if i.isnumeric()])>0)) else 0, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BCOKfKucFvv6"
   },
   "outputs": [],
   "source": [
    "##Similarity features\n",
    "print(\"step0 \")\n",
    "load_embeddings()\n",
    "#subset['rootword_match']=subset.apply(lambda x: col2.root_word_match(x),axis=1)\n",
    "#subset['ner_match']=subset.apply(lambda x: col2.ner_match(x), axis=1)\n",
    "print(\"step1 \")\n",
    "subset['unigrams_match']=subset.apply(lambda x: col2.match(x['ques_unigram'],x['sent_unigram']), axis=1)\n",
    "print(\"step2 \")\n",
    "subset['bigrams_match']=subset.apply(lambda x: col2.match(x['ques_bigram'],x['sent_bigram']), axis=1)\n",
    "print(\"step3 \")\n",
    "subset['trigrams_match']=subset.apply(lambda x: col2.match(x['ques_trigram'],x['sent_trigram']), axis=1)\n",
    "#subset['pos_tags_match']=subset.apply(lambda x: col2.pos_tags_match(x), axis=1)\n",
    "#subset['ques_person']=subset.apply(lambda x: col2.ques_specific_ner(x,'Person'), axis=1)\n",
    "#subset['ques_org']=subset.apply(lambda x: col2.ques_specific_ner(x,'Organisation'), axis=1)\n",
    "#subset['ques_numerical']=subset.apply(lambda x: col2.ques_specific_ner(x,'Numerical'), axis=1)\n",
    "#subset['ques_location']=subset.apply(lambda x: col2.ques_specific_ner(x,'Location'), axis=1)\n",
    "#subset['ques_datetime']=subset.apply(lambda x: col2.ques_specific_ner(x,'Datetime'), axis=1)\n",
    "#subset['sent_person']=subset.apply(lambda x: col2.sent_specific_ner(x,'Person'), axis=1)\n",
    "#subset['sent_org']=subset.apply(lambda x: col2.sent_specific_ner(x,'Organisation'), axis=1)\n",
    "#subset['sent_numerical']=subset.apply(lambda x: col2.sent_specific_ner(x,'Numerical'), axis=1)\n",
    "#subset['sent_location']=subset.apply(lambda x: col2.sent_specific_ner(x,'Location'), axis=1)\n",
    "#subset['sent_datetime']=subset.apply(lambda x: col2.sent_specific_ner(x,'Datetime'), axis=1)\n",
    "subset['ratio_length']=subset['sent_length']/subset['ques_length']\n",
    "\n",
    "print(\"step3 \")\n",
    "subset['ques_embeddings']=subset['ques_new'].apply(lambda x: get_embeddings(x))\n",
    "print(\"step4 \")\n",
    "subset['sent_embeddings']=subset['sent_new'].apply(lambda x: get_embeddings(x))\n",
    "print(\"step5 \")\n",
    "subset['ques_embeddings']=subset['ques_embeddings'].apply(lambda x: np.array(x).mean(axis=0))\n",
    "print(\"step6 \")\n",
    "subset['sent_embeddings']=subset['sent_embeddings'].apply(lambda x: np.array(x).mean(axis=0))\n",
    "print(\"step7 \")\n",
    "subset['cos_similarity']=subset.apply(lambda x: cosine(x['ques_embeddings'],x['sent_embeddings']), axis=1)\n",
    "print(\"step8 \")\n",
    "subset['eu_similarity']=subset.apply(lambda x: np.sqrt(sum((x['ques_embeddings']-x['sent_embeddings'])**2)), axis=1)\n",
    "print(\"step 9\")\n",
    "subset['jac_similarity']=subset.apply(lambda x: col2.get_jaccard_sim(x['ques_new_ws'],x['sent_new_ws']), axis=1 )\n",
    "\n",
    "\n",
    "del subset['ques_embeddings']\n",
    "del subset['sent_embeddings']\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OuFw1wip4FnK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yv2rz2VtFv0K",
    "outputId": "6c3bb21a-77da-4a4b-ee7a-3269b9472b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.to_csv(\"test_adV2.tsv\", sep=\"\\t\", index=False)\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MZH3g7jbFv73"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4ibjvyIBFv_0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6gNxFTtEFwD7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3eAv7dt1FwHy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AP4oiUbNFwL5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sEgNKzePFwQE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ad_creationv2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
